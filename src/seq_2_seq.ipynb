{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import os\n",
    "import io\n",
    "import string\n",
    "import tarfile\n",
    "import pickle as pkl\n",
    "from urllib.request import urlopen\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from tensorflow.data import Dataset\n",
    "from tensorflow.keras.layers import Bidirectional, GRU, Dense, Dropout, Embedding, InputLayer\n",
    "from tensorflow.keras import Model\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.losses import SparseCategoricalCrossentropy\n",
    "\n",
    "pd.set_option(\"display.max_colwidth\", 1000)\n",
    "tf.test.is_gpu_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_DIRECTORY = os.path.join(os.path.dirname(os.getcwd()), \"data\")\n",
    "DATA_DIRECTORY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EuroParlEnFr:\n",
    "    URL = \"http://statmt.org/europarl/v7/fr-en.tgz\"\n",
    "    FR = \"europarl-v7.fr-en.fr\"\n",
    "    EN = \"europarl-v7.fr-en.en\"\n",
    "\n",
    "    def load(self):\n",
    "        self._download()\n",
    "        with open(os.path.join(DATA_DIRECTORY, self.EN), \"r\") as f:\n",
    "            en = f.readlines()\n",
    "        with open(os.path.join(DATA_DIRECTORY, self.FR), \"r\") as f:\n",
    "            fr = f.readlines()\n",
    "        return pd.DataFrame(zip(en, fr), columns=[\"en\", \"fr\"])\n",
    "\n",
    "    def _download(self):\n",
    "        if all(os.path.exists(os.path.join(DATA_DIRECTORY, f)) for f in (self.FR, self.EN)):\n",
    "            print(\"Data has already been downloaded.\")\n",
    "            return\n",
    "        os.makedirs(DATA_DIRECTORY, exist_ok=True)\n",
    "        print(f\"Downloading : {self.URL}\")\n",
    "        with urlopen(self.URL) as response:\n",
    "            tf = tarfile.open(fileobj=io.BytesIO(response.read()))\n",
    "        tf.extractall(path=DATA_DIRECTORY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_sentence(s):\n",
    "    s = s.lower()\n",
    "    s = re.sub(f'([{string.punctuation}])', r' \\1 ', s)\n",
    "    s = re.sub(f'\\s+', r' ', s)\n",
    "    return \"<sos> \" + s.strip() + \" <eos>\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_tokenizer(sentences):\n",
    "    tokenizer = Tokenizer(filters=\"\", oov_token=\"<unk>\")\n",
    "    tokenizer.fit_on_texts(sentences)\n",
    "    return tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_tokenizer_and_preprocessed_files(dataset):\n",
    "    df = dataset.load()\n",
    "    df[\"en\"] = df[\"en\"].apply(preprocess_sentence)\n",
    "    df[\"fr\"] = df[\"fr\"].apply(preprocess_sentence)\n",
    "    df.to_csv(os.path.join(DATA_DIRECTORY, \"preprocessed.csv\"), sep=\"|\", index=False)\n",
    "    df[\"en\"] = df[\"en\"].apply(lambda s: s.split())\n",
    "    df[\"fr\"] = df[\"fr\"].apply(lambda s: s.split())\n",
    "    with open(os.path.join(DATA_DIRECTORY, \"en_tokenizer.pkl\"), \"wb\") as en_t, open(os.path.join(DATA_DIRECTORY, \"fr_tokenizer.pkl\"), \"wb\") as fr_t:\n",
    "        pkl.dump(create_tokenizer(df[\"en\"]), en_t)\n",
    "        pkl.dump(create_tokenizer(df[\"fr\"]), fr_t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_tokenizers():\n",
    "    with open(os.path.join(DATA_DIRECTORY, \"en_tokenizer.pkl\"), \"rb\") as en_t, open(os.path.join(DATA_DIRECTORY, \"fr_tokenizer.pkl\"), \"rb\") as fr_t:\n",
    "        return pkl.load(en_t), pkl.load(fr_t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_sequence_example(en, fr):\n",
    "    ex = tf.train.SequenceExample()\n",
    "    en_feature = ex.feature_lists.feature_list[\"en\"]\n",
    "    fr_feature = ex.feature_lists.feature_list[\"fr\"]\n",
    "    for token in en:\n",
    "        en_feature.feature.add().int64_list.value.append(token)\n",
    "    for token in fr:\n",
    "        fr_feature.feature.add().int64_list.value.append(token)\n",
    "    return ex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_tf_records():\n",
    "    en_tokenizer, fr_tokenizer = load_tokenizers()\n",
    "    with tf.io.TFRecordWriter(os.path.join(DATA_DIRECTORY, \"europarlenfr.tfrecord\")) as writer:\n",
    "        for i, chunk in enumerate(pd.read_csv(os.path.join(DATA_DIRECTORY, \"preprocessed.csv\"), sep=\"|\", chunksize=100000)):\n",
    "            print(f\"Processing chunk {i}\")\n",
    "            chunk[\"en\"] = chunk[\"en\"].apply(lambda s: s.split())\n",
    "            chunk[\"fr\"] = chunk[\"fr\"].apply(lambda s: s.split())\n",
    "            chunk[\"en\"] = en_tokenizer.texts_to_sequences(chunk[\"en\"])\n",
    "            chunk[\"fr\"] = en_tokenizer.texts_to_sequences(chunk[\"fr\"])\n",
    "            for _, row in chunk.iterrows():\n",
    "                writer.write(make_sequence_example(row['en'], row['fr']).SerializeToString())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_example_proto(ex):\n",
    "    sequence_features = {\n",
    "        \"en\": tf.io.FixedLenSequenceFeature([], dtype=tf.int64),\n",
    "        \"fr\": tf.io.FixedLenSequenceFeature([], dtype=tf.int64)\n",
    "    }\n",
    "    _, sequence = tf.io.parse_single_sequence_example(ex, sequence_features=sequence_features)\n",
    "    return sequence[\"en\"], sequence[\"fr\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_datasets(batch_size):\n",
    "    dataset = tf.data.TFRecordDataset(filenames=[os.path.join(DATA_DIRECTORY, \"europarlenfr.tfrecord\")])\n",
    "    num_examples = sum(1 for _ in dataset)\n",
    "    dataset = dataset.map(parse_example_proto)\n",
    "    dataset = dataset.shuffle(buffer_size=num_examples)\n",
    "    split = (num_examples * 95) // 100\n",
    "    dataset_train = dataset.take(split).padded_batch(batch_size, padded_shapes=([None],[None]), drop_remainder=False)\n",
    "    dataset_val = dataset.skip(split).padded_batch(batch_size, padded_shapes=([None],[None]), drop_remainder=False)\n",
    "    return dataset_train, dataset_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(Model):\n",
    "    def __init__(self, vocab_size):\n",
    "        super().__init__()\n",
    "        self.embedding = Embedding(input_dim=vocab_size, output_dim=300)\n",
    "        self.gru = Bidirectional(GRU(units=256))\n",
    "    \n",
    "    def call(self, X, hidden):\n",
    "        embedded = self.embedding(X)\n",
    "        return self.gru(embedded, hidden)\n",
    "    \n",
    "    def initial_hidden_state(self, batch_size):\n",
    "        return [tf.zeros((batch_size, 256))] * 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(Model):\n",
    "    def __init__(self, vocab_size):\n",
    "        super().__init__()\n",
    "        self.embedding = Embedding(vocab_size, 300)\n",
    "        self.gru = GRU(512, return_sequences=False, return_state=False)\n",
    "        self.dense = Dense(vocab_size, activation='softmax')\n",
    "    \n",
    "    def call(self, X, hidden):\n",
    "        embedded = self.embedding(X)\n",
    "        output = self.gru(embedded, hidden)\n",
    "        return self.dense(output), output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_loss(loss_fn, y_true, y_pred):\n",
    "    mask = tf.math.logical_not(tf.math.equal(y_true, 0))\n",
    "    loss = loss_fn(y_true, y_pred)\n",
    "    mask = tf.cast(mask, dtype=loss.dtype)\n",
    "    loss *= mask\n",
    "    return tf.reduce_mean(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(encoder, decoder, en_tokenizer, fr_tokenizer, dataset_train, dataset_val):\n",
    "    optimizer = Adam()\n",
    "    loss_fn = SparseCategoricalCrossentropy()\n",
    "    for epoch in range(10):\n",
    "        for (batch, (en, fr)) in enumerate(dataset_train):\n",
    "            batch_size = en.shape[0]\n",
    "            encoder_hidden_state = encoder.initial_hidden_state(batch_size)\n",
    "            loss = 0\n",
    "            with tf.GradientTape() as tape:\n",
    "                context = encoder(en, encoder_hidden_state)\n",
    "                decoder_hidden_state = context\n",
    "                decoder_input = tf.expand_dims([fr_tokenizer.word_index['<sos>']] * batch_size, 1)\n",
    "                for i in range(1, fr.shape[1]):\n",
    "                    predictions, decoder_hidden_state = decoder(decoder_input, decoder_hidden_state)\n",
    "                    loss += calculate_loss(loss_fn, fr[:, i], predictions)\n",
    "                    decoder_input = tf.expand_dims(fr[:, i], 1)\n",
    "            variables = encoder.trainable_variables + decoder.trainable_variables\n",
    "            gradients = tape.gradient(loss, variables)\n",
    "            optimizer.apply_gradients(zip(gradients, variables))\n",
    "            print(f\"Epoch: {epoch+1} | Batch: {batch+1} | Loss: {loss / batch_size}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "create_tokenizer_and_preprocessed_files(EuroParlEnFr())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "create_tf_records()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_train, dataset_val = create_datasets(32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "en_tokenizer, fr_tokenizer = load_tokenizers()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder = Encoder(len(en_tokenizer.word_index))\n",
    "decoder = Decoder(len(fr_tokenizer.word_index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train(encoder, decoder, en_tokenizer, fr_tokenizer, dataset_train, dataset_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
