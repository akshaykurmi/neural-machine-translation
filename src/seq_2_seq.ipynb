{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pickle as pkl\n",
    "\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Bidirectional, GRU, Dense, Dropout, Embedding, InputLayer\n",
    "from tensorflow.keras import Model\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.losses import SparseCategoricalCrossentropy\n",
    "\n",
    "from data import ManyThingsEnFr, create_tokenizer_and_preprocessed_files, create_tf_records\n",
    "from data import load_datasets, load_tokenizers\n",
    "from data import zip_directory_and_upload_to_gcs, download_from_gcs_and_unzip_directory\n",
    "\n",
    "pd.set_option(\"display.max_colwidth\", 1000)\n",
    "print(tf.__version__, tf.test.is_gpu_available())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BASE_DIR = os.path.dirname(os.getcwd())\n",
    "DATA_DIRECTORY = os.path.join(BASE_DIR, \"data\")\n",
    "LOG_DIRECTORY = os.path.join(BASE_DIR, \"logs\")\n",
    "CHECKPOINT_DIRECTORY = os.path.join(BASE_DIR, \"checkpoints\")\n",
    "PREFIX = \"manythingsenfr\"\n",
    "GCS_BUCKET = \"nmt-data-store\"\n",
    "os.environ[\"GOOGLE_APPLICATION_CREDENTIALS\"] = os.path.join(BASE_DIR, \"nmt-gcs-credentials.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create_tokenizer_and_preprocessed_files(ManyThingsEnFr(DATA_DIRECTORY), DATA_DIRECTORY, PREFIX)\n",
    "# create_tf_records(DATA_DIRECTORY, PREFIX)\n",
    "# zip_directory_and_upload_to_gcs(DATA_DIRECTORY, GCS_BUCKET, \"manythings.enfr.zip\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "download_from_gcs_and_unzip_directory(DATA_DIRECTORY, GCS_BUCKET, \"manythings.enfr.zip\")\n",
    "download_from_gcs_and_unzip_directory(os.path.join(CHECKPOINT_DIRECTORY, \"run_1\"), GCS_BUCKET, f\"checkpoints.run_1.zip\")\n",
    "download_from_gcs_and_unzip_directory(os.path.join(LOG_DIRECTORY, \"run_1\"), GCS_BUCKET, f\"logs.run_1.zip\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(Model):\n",
    "    def __init__(self, vocab_size):\n",
    "        super().__init__()\n",
    "        self.embedding = Embedding(input_dim=vocab_size, output_dim=300)\n",
    "        self.gru = Bidirectional(GRU(units=256))\n",
    "    \n",
    "    def call(self, X, hidden):\n",
    "        embedded = self.embedding(X)\n",
    "        return self.gru(embedded, hidden)\n",
    "    \n",
    "    def initial_hidden_state(self, batch_size):\n",
    "        return [tf.zeros((batch_size, 256))] * 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(Model):\n",
    "    def __init__(self, vocab_size):\n",
    "        super().__init__()\n",
    "        self.embedding = Embedding(vocab_size, 300)\n",
    "        self.gru = GRU(512, return_sequences=False, return_state=False)\n",
    "        self.dense = Dense(vocab_size, activation='softmax')\n",
    "    \n",
    "    def call(self, X, hidden):\n",
    "        embedded = self.embedding(X)\n",
    "        output = self.gru(embedded, hidden)\n",
    "        return self.dense(output), output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_loss(loss_fn, y_true, y_pred):\n",
    "    mask = tf.math.logical_not(tf.math.equal(y_true, 0))\n",
    "    loss = loss_fn(y_true, y_pred)\n",
    "    mask = tf.cast(mask, dtype=loss.dtype)\n",
    "    loss *= mask\n",
    "    return tf.reduce_mean(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_on_batch(en, fr, encoder, decoder, en_tokenizer, fr_tokenizer, loss_fn, optimizer):\n",
    "    batch_size = en.shape[0]\n",
    "    encoder_hidden_state = encoder.initial_hidden_state(batch_size)\n",
    "    loss = 0\n",
    "    with tf.GradientTape() as tape:\n",
    "        context = encoder(en, encoder_hidden_state)\n",
    "        decoder_hidden_state = context\n",
    "        decoder_input = tf.expand_dims([fr_tokenizer.word_index['<sos>']] * batch_size, 1)\n",
    "        for i in range(1, fr.shape[1]):\n",
    "            predictions, decoder_hidden_state = decoder(decoder_input, decoder_hidden_state)\n",
    "            loss += calculate_loss(loss_fn, fr[:, i], predictions)\n",
    "            decoder_input = tf.expand_dims(fr[:, i], 1)\n",
    "    variables = encoder.trainable_variables + decoder.trainable_variables\n",
    "    gradients = tape.gradient(loss, variables)\n",
    "    optimizer.apply_gradients(zip(gradients, variables))\n",
    "    return loss / batch_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_checkpoint(encoder, decoder, optimizer, run_id):\n",
    "    ckpt = tf.train.Checkpoint(step=tf.Variable(0), optimizer=optimizer, encoder=encoder, decoder=decoder)\n",
    "    ckpt_manager = tf.train.CheckpointManager(ckpt, os.path.join(CHECKPOINT_DIRECTORY, run_id), max_to_keep=5)\n",
    "    ckpt.restore(ckpt_manager.latest_checkpoint)\n",
    "    if ckpt_manager.latest_checkpoint:\n",
    "        print(\"Restored from {}\".format(ckpt_manager.latest_checkpoint))\n",
    "    return ckpt, ckpt_manager"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(encoder, decoder, en_tokenizer, fr_tokenizer, dataset_train, dataset_val, run_id, log_directory):\n",
    "    optimizer = Adam()\n",
    "    loss_fn = SparseCategoricalCrossentropy()\n",
    "    summary_writer = tf.summary.create_file_writer(os.path.join(log_directory, run_id))\n",
    "    ckpt, ckpt_manager = init_checkpoint(encoder, decoder, optimizer, run_id)\n",
    "    for epoch in range(10):\n",
    "        for (batch, (en, fr)) in enumerate(dataset_train):\n",
    "            loss = train_on_batch(en, fr, encoder, decoder, en_tokenizer, fr_tokenizer, loss_fn, optimizer)\n",
    "            if int(ckpt.step) % 1 == 0:\n",
    "                print(f\"Epoch: {epoch+1} | Batch: {batch+1} | Loss: {loss}\")\n",
    "            ckpt.step.assign_add(1)\n",
    "            if int(ckpt.step) % 1 == 0:\n",
    "                save_path = ckpt_manager.save()\n",
    "                print(f\"Saved checkpoint {save_path}\")\n",
    "            with summary_writer.as_default():\n",
    "                tf.summary.scalar('loss', loss, step=int(ckpt.step))\n",
    "        zip_directory_and_upload_to_gcs(os.path.join(LOG_DIRECTORY, run_id), GCS_BUCKET, f\"logs.{run_id}.zip\")\n",
    "        zip_directory_and_upload_to_gcs(os.path.join(CHECKPOINT_DIRECTORY, run_id), GCS_BUCKET, f\"checkpoints.{run_id}.zip\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_train, dataset_val = load_datasets(32, DATA_DIRECTORY, PREFIX)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "en_tokenizer, fr_tokenizer = load_tokenizers(DATA_DIRECTORY, PREFIX)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder = Encoder(len(en_tokenizer.word_index) + 1)\n",
    "decoder = Decoder(len(fr_tokenizer.word_index) + 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train(encoder, decoder, en_tokenizer, fr_tokenizer, dataset_train, dataset_val, \"run_1\", LOG_DIRECTORY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
