{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import string\n",
    "import os\n",
    "import io\n",
    "import tarfile\n",
    "from urllib.request import urlopen\n",
    "\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from tensorflow.data import Dataset\n",
    "from tensorflow.keras.layers import Bidirectional, GRU, Dense, Dropout, Embedding, InputLayer\n",
    "from tensorflow.keras import Model\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.losses import SparseCategoricalCrossentropy\n",
    "\n",
    "pd.set_option(\"display.max_colwidth\", 1000)\n",
    "tf.test.is_gpu_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_DIRECTORY = os.path.join(os.path.dirname(os.getcwd()), \"data\")\n",
    "DATA_DIRECTORY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EuroParlEnFr:\n",
    "    URL = \"http://statmt.org/europarl/v7/fr-en.tgz\"\n",
    "    FR = \"europarl-v7.fr-en.fr\"\n",
    "    EN = \"europarl-v7.fr-en.en\"\n",
    "\n",
    "    def load(self):\n",
    "        with open(os.path.join(DATA_DIRECTORY, self.EN), \"r\") as f:\n",
    "            en = f.readlines()\n",
    "        with open(os.path.join(DATA_DIRECTORY, self.FR), \"r\") as f:\n",
    "            fr = f.readlines()\n",
    "        return pd.DataFrame(zip(en, fr), columns=[\"en\", \"fr\"])\n",
    "\n",
    "    def download(self):\n",
    "        if all(os.path.exists(os.path.join(DATA_DIRECTORY, f)) for f in (self.FR, self.EN)):\n",
    "            print(\"Data has already been downloaded.\")\n",
    "            return\n",
    "        os.makedirs(DATA_DIRECTORY, exist_ok=True)\n",
    "        print(f\"Downloading : {self.URL}\")\n",
    "        with urlopen(self.URL) as response:\n",
    "            tf = tarfile.open(fileobj=io.BytesIO(response.read()))\n",
    "        tf.extractall(path=DATA_DIRECTORY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_sentence(s):\n",
    "    s = s.lower()\n",
    "    s = re.sub(f'([{string.punctuation}])', r' \\1 ', s)\n",
    "    s = re.sub(f'\\s+', r' ', s)\n",
    "    return ['<sos>'] + s.strip().split() + ['<eos>']\n",
    "\n",
    "def preprocess_sentence_pairs(df):\n",
    "    for lang in [\"en\", \"fr\"]:\n",
    "        df[lang] = df[lang].apply(preprocess_sentence)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(sentences):\n",
    "    tokenizer = Tokenizer(filters=\"\", oov_token=\"<unk>\")\n",
    "    tokenizer.fit_on_texts(sentences)\n",
    "    sequences = tokenizer.texts_to_sequences(sentences)\n",
    "    sequences = pad_sequences(sequences, padding='post', value=0.0)\n",
    "    return tokenizer, sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_datasets(en_sequences, fr_sequences, batch_size):\n",
    "    num_sequences = en_sequences.shape[0]\n",
    "    dataset = tf.data.Dataset.from_tensor_slices((en_sequences, fr_sequences))\n",
    "    dataset = dataset.shuffle(buffer_size=num_sequences)\n",
    "    split = (num_sequences * 95) // 100\n",
    "    dataset_train = dataset.take(split).batch(batch_size, drop_remainder=True)\n",
    "    dataset_val = dataset.skip(split).batch(batch_size, drop_remainder=True)\n",
    "    return dataset_train, dataset_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = EuroParlEnFr()\n",
    "dataset.download()\n",
    "sentence_pairs = dataset.load()\n",
    "sentence_pairs = preprocess_sentence_pairs(sentence_pairs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "en_tokenizer, en_sequences = tokenize(sentence_pairs[\"en\"])\n",
    "fr_tokenizer, fr_sequences = tokenize(sentence_pairs[\"fr\"])\n",
    "dataset_train, dataset_val = create_datasets(en_sequences, fr_sequences, 32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(Model):\n",
    "    def __init__(self, vocab_size):\n",
    "        super().__init__()\n",
    "        self.embedding = Embedding(input_dim=vocab_size, output_dim=300)\n",
    "        self.gru = Bidirectional(GRU(units=256))\n",
    "    \n",
    "    def call(self, X, hidden):\n",
    "        embedded = self.embedding(X)\n",
    "        return self.gru(embedded, hidden)\n",
    "    \n",
    "    def initial_hidden_state(self, batch_size):\n",
    "        return [tf.zeros((batch_size, 256))] * 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(Model):\n",
    "    def __init__(self, vocab_size):\n",
    "        super().__init__()\n",
    "        self.embedding = Embedding(vocab_size, 300)\n",
    "        self.gru = GRU(512, return_sequences=False, return_state=False)\n",
    "        self.dense = Dense(vocab_size, activation='softmax')\n",
    "    \n",
    "    def call(self, X, hidden):\n",
    "        embedded = self.embedding(X)\n",
    "        output = self.gru(embedded, hidden)\n",
    "        return self.dense(output), output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_loss(loss_fn, y_true, y_pred):\n",
    "    mask = tf.math.logical_not(tf.math.equal(y_true, 0))\n",
    "    loss = loss_fn(y_true, y_pred)\n",
    "    mask = tf.cast(mask, dtype=loss.dtype)\n",
    "    loss *= mask\n",
    "    return tf.reduce_mean(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(encoder, decoder, en_tokenizer, fr_tokenizer, dataset_train, dataset_val):\n",
    "    optimizer = Adam()\n",
    "    loss_fn = SparseCategoricalCrossentropy()\n",
    "    \n",
    "    BATCH_SIZE = 32\n",
    "    \n",
    "    for epoch in range(10):\n",
    "        encoder_hidden_state = encoder.initial_hidden_state(32)\n",
    "        for (batch, (en, fr)) in enumerate(dataset_train):\n",
    "            loss = 0\n",
    "            with tf.GradientTape() as tape:\n",
    "                context = encoder(en, encoder_hidden_state)\n",
    "                decoder_hidden_state = context\n",
    "                decoder_input = tf.expand_dims([fr_tokenizer.word_index['<sos>']] * BATCH_SIZE, 1)\n",
    "                for i in range(1, fr.shape[1]):\n",
    "                    predictions, decoder_hidden_state = decoder(decoder_input, decoder_hidden_state)\n",
    "                    loss += calculate_loss(loss_fn, fr[:, i], predictions)\n",
    "                    decoder_input = tf.expand_dims(fr[:, i], 1)\n",
    "            variables = encoder.trainable_variables + decoder.trainable_variables\n",
    "            gradients = tape.gradient(loss, variables)\n",
    "            optimizer.apply_gradients(zip(gradients, variables))\n",
    "            print(f\"Epoch: {epoch+1} | Batch: {batch+1} | Loss: {loss / BATCH_SIZE}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder = Encoder(len(en_tokenizer.word_index))\n",
    "decoder = Decoder(len(fr_tokenizer.word_index))\n",
    "train(encoder, decoder, en_tokenizer, fr_tokenizer, dataset_train, dataset_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
